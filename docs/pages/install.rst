==================================================================
Installation
==================================================================

Local development workflow
-----

Pull x86_64 image that was built during CI/CD process or build image locally

# Option 1: Pull built image (check registry tab on your GitLab project web page for url)

::

docker pull <image-url>

# Option 2: Build image on your local machine
python3 <bonseyes_aiasset_name>/docker/build.py \
    --platform x86_64 \
    --profile <bonseyes_aiasset_name>/docker/profiles/x86_64/ubuntu18.04_cuda10.2_python3.6_tensorrt7.0.yml \
    --image-name <bonseyes_aiasset_name>x86_64:<v1.0>_cuda10.2_tensorrt7.0 \
    --device cuda

build script calls Dockerfile on specified platform (x86_64, Jetson devices and RaspberryPi4) and device (GPU or CPU). In Dockerfile you should run your setup.py script which contains all python packages with their versions used in your family_accounting for x86_64, Jetson devices and RaspberryPi4.

Dockerfiles for x86_64, Jetson and RaspberryPi4 are stored in /<bonseyes_aiasset_name>/docker/platforms/

Pytorch, CMake, OpenCV, ONNXRuntime, ONNX, TensorRT, Python versions which will be installed during docker build are written in /<bonseyes_aiasset_name>/docker/profiles/. These versions are sent as arguments to Dockerfiles.

Existing x86_64 profiles:

ubuntu18.04_cuda10.2_python3.6_tensorrt7.0.yml
ubuntu20.04_cuda11.4_python3.8_tensorrt8.0.yml
ubuntu18.04_python3.7.yml
Existing NVIDIA Jetson profiles:

jetpack4.4.yml
jetpack4.6.yml
For RaspberryPi4 available profile is:

rpi_arm64v8.yml
The result of build script (if everything works properly) is new docker image.

Note

If you want to make minor changes (very small changes from official code instead of writing in your Bonseyes family_accounting) in submodule, you mustnâ€™t commit changes to official source repository. Instead of committing changes to official repository, you need to create git patch and save it to /source/patch/ directory. To apply patch to submodule use command in your container:

cd /app/source/<submodule_name> && git apply /app/source/patch/modification_1.patch
You also need to add this command in Dockerfile for building image with applying patch before setup.

Run x86_64 image and mount your project root to /app

If you are using directory with images and annotations generated by DataTool, you should mount directory with datasets and annotations to <bonseyes_aiasset_name>/data/storage directory while executing docker run command. In this case you should run built container with:

# Example how to run built container when you are using dataset and its annotations generated by DataTool
cd <bonseyes_aiasset_name>

docker run --name <bonseyes_aiasset_name> \
    --privileged --rm -it \
    --gpus 0 \
    --ipc=host \
    -p 8888:8888 \
    -v $(pwd):/app \
    -v /path/to/processed/dataset1:/app/<bonseyes_aiasset_name>/data/dataset1 \
    -v /path/to/custom_base_types.py:/app/<bonseyes_aiasset_name>/data/dataset1/custom_base_types.py \
    -v /path/to/custom_dataset_model.py:/app/<bonseyes_aiasset_name>/data/dataset1/custom_dataset_model.py \
    -v /path/to/processed/dataset2:/app/<bonseyes_aiasset_name>/data/dataset2 \
    -v /path/to/custom_base_types.py:/app/<bonseyes_aiasset_name>/data/dataset2/custom_base_types.py \
    -v /path/to/custom_dataset_model.py:/app/<bonseyes_aiasset_name>/data/dataset2/custom_dataset_model.py \
    -v /tmp/.X11-unix:/tmp/.X11-unix \
    --device /dev/video0 \
    -e DISPLAY=$DISPLAY \
    <bonseyes_aiasset_name>x86_64:<v1.0>_cuda10.2_tensorrt7.0
At this point you can develop on your host environment using IDE of your choice and test implementation inside of running docker container
